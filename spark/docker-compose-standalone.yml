version: '3.0'
services:
  spark-master0:
    build:
      context: .
      dockerfile: DockerfileWithLivy
    image: hongchhe/sparkwithlivy
    container_name: spark-master0
    hostname: spark-master0
    environment:
      - SPARK_TYPE=master
      - SPARK_MASTER_URL=spark:\/\/spark-master0:7077
      - START_SPARK_HISTORY_SERVER=true
      - SPARK_HISTORYLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/spark-events
      - SPARK_EVENTLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/eventlog
      - SPARK_DRIVER_MAXRESULTSIZE=4g
      - SPARK_DRIVER_MEMORY=4g
      - SPARK_EXECUTOR_MEMORY=4g
      - WORKER_LIST=spark-worker0 \nspark-worker1
      - HDFS_HOST=spark-master0
      - SLAVE_LIST=spark-worker0 \nspark-worker1
      - HDFS_TYPE=namenode
      - YARN_TYPE=none
      - HDFS_REPLICA=2
      - START_LIVY_SERVER=true
      - SPARK_DYNAMIC_ALLOCATION_ENABLED=true
      - SPARK_SHUFFLE_SERVICE_ENABLED=true
    volumes:
      - ./myvol:/myvol
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    ports:
      - 8080:8080 # masterWebUI
      - 7077:7077 # master service
      - 6066:6066 # REST server
      - 18080:18080 # history server
      - 4040:4040 # driverProgramWebUI
      - 4041:4041
      - 4042:4042
      - 4042:4043
      - 4042:4044
      - 4042:4045
      - 8888:8888 # ipython notebook
      # HDFS ENV
      - 50070:50070 # name node port
      - 9000:9000
      - 8998:8998
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 1m30s
      timeout: 10s
      retries: 3
    networks:
      - net_hadoop

  spark-worker0:
    build: .
    image: hongchhe/spark
    container_name: spark-worker0
    hostname: spark-worker0
    environment:
      - SPARK_TYPE=slave
      - SPARK_MASTER_URL=spark:\/\/spark-master0:7077
      - WORKER_LIST=spark-worker0 \nspark-worker1
      - SPARK_HISTORYLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/spark-events
      - SPARK_EVENTLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/eventlog
      # HDFS ENV
      - HDFS_HOST=spark-master0
      - SLAVE_LIST=spark-worker0 \nspark-worker1
      - HDFS_TYPE=datanode
      - YARN_TYPE=none
      - HDFS_REPLICA=2
      - SPARK_DYNAMIC_ALLOCATION_ENABLED=true
      - SPARK_SHUFFLE_SERVICE_ENABLED=true
    volumes:
      - ./myvol:/myvol
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    ports:
      - 8081:8081 # workerUI
      - 6066
      # HDFS ports
      - 50010:50010
      - 50020:50020
      - 50075:50075
      - 8088  # resource manager webapp port
      - 19888 # mapreduce job history webapp server
    networks:
      - net_hadoop

  spark-worker1:
    image: hongchhe/spark
    container_name: spark-worker1
    hostname: spark-worker1
    environment:
      - SPARK_TYPE=slave
      - SPARK_MASTER_URL=spark:\/\/spark-master0:7077
      - WORKER_LIST=spark-worker0 \nspark-worker1
      - SPARK_HISTORYLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/spark-events
      - SPARK_EVENTLOG_DIR=hdfs:\/\/spark-master0:9000\/spark\/eventlog
      # HDFS ENV
      - HDFS_HOST=spark-master0
      - SLAVE_LIST=spark-worker0 \nspark-worker1
      - HDFS_TYPE=datanode
      - YARN_TYPE=none
      - HDFS_REPLICA=2
      - SPARK_DYNAMIC_ALLOCATION_ENABLED=true
      - SPARK_SHUFFLE_SERVICE_ENABLED=true
    volumes:
      - ./myvol:/myvol
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    ports:
      - 8091:8081 # workerUI
      - 6066
      # HDFS ports
      - 50011:50010
      - 50021:50020
      - 50076:50075
      - 8088  # resource manager webapp port
      - 19888 # mapreduce job history webapp server
    networks:
      - net_hadoop

  oracle11g:
    image: wnameless/oracle-xe-11g
    container_name: oracle11g
    hostname: oracle11g
    environment:
      - ORACLE_ALLOW_REMOTE=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    cap_add:
      - IPC_LOCK
    ports:
      - 49161:1521
    networks:
      - net_hadoop

#  sparkjob:
#    image: sparkjobserver:2.1.1
#    container_name: sparkjob
#    hostname: sparkjob
#    environment:
#      - SPARK_MASTER_URL=spark:\/\/spark-master0:7077
#    ulimits:
#      memlock:
#        soft: -1
#        hard: -1
#      nofile:
#        soft: 65536
#        hard: 65536
#    cap_add:
#      - IPC_LOCK
#    ports:
#      - 8090:8090
#    networks:
#      - net_hadoop
networks:
  net_hadoop:
    driver: bridge
